{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfx_practice_pt1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obkQexStS-G3"
      },
      "source": [
        "# Hands-on Exercise Part 1: TFX Pipeline components\n",
        "\n",
        "With this notebook we will define, run and inspect the individual components of a TFX pipeline. All the code should run without any modifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjhVZbWSWKsM"
      },
      "source": [
        "### Install and import libraries\n",
        "\n",
        "Before getting started, we need to pip install tfx. The will take a minute. Make sure to *RESTART RUNTIME* when the installation is complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN9NyTSOD9p3"
      },
      "source": [
        "! pip install -U tfx"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EimZ97keETeb"
      },
      "source": [
        "import os\n",
        "import tfx\n",
        "from tfx import v1 as tfx_v1\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Tuner\n",
        "from tfx.components import Transform\n",
        "from tfx.dsl.components.common import resolver\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from  tfx.proto import example_gen_pb2\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "from google.protobuf import text_format\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6sU1aPZNA3"
      },
      "source": [
        "## Set up directories and load the data\n",
        "\n",
        "For this exercise we'll be using the [Laptop Prices Prediction dataset](https://www.kaggle.com/danielbethell/laptop-prices-prediction). Goal will be to create a workflow for a model that predicts laptop price based on some descriptive attributes. The dataset is fairly small with 1303 records and 12 features which allows for quick load/transform/training times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQUStUApETl9"
      },
      "source": [
        "# Set up the directories for your pipeline\n",
        "DATA_ROOT = 'data'                                       # location of the raw CSV data set\n",
        "SERVING_MODEL_DIR = 'serving_model'                      # where we will save the final model for deployment with TF serving\n",
        "PIPELINE_NAME = 'laptop_pipeline'\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME) # to store all the artifacts that our pipeline components will create\n",
        "\n",
        "! mkdir {DATA_ROOT}\n",
        "! mkdir {SERVING_MODEL_DIR}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQqQz42QSrAT"
      },
      "source": [
        "# download the raw dataset\n",
        "! gdown https://drive.google.com/uc?id=1NzyT0YLV9xk71jDR_lde6rKifLB7UT9p"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2M62NX8MW_B"
      },
      "source": [
        "Let's save the data n the correct root folder and take a quick look at a few records:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp44OIM0Egt1"
      },
      "source": [
        "file_dir = 'laptop_price.csv'\n",
        "ds = pd.read_csv(file_dir, encoding=\"iso-8859-1\")\n",
        "ds.to_csv(os.path.join(DATA_ROOT, 'train.csv'), index=False, encoding=\"utf-8\")\n",
        "\n",
        "ds.head()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-twLqn4Z-H7"
      },
      "source": [
        "## InteractiveContex - Our pipeline orchestrator for development\n",
        "The key difference between running TFX in production vs. in this experimental notebook setting is the orchestrator. While in production your pipeline will be orchestrated by Apache Beam , Apache Airflow or Kubeflow, here we create an **InteractiveContext** that manages component execution in the notebook and allows us to inspect the created artifacts.\n",
        "\n",
        "It also creates an in-memory ML Metadata store using SQLite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s743W4hREgwc"
      },
      "source": [
        "context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciNhanFtcV6h"
      },
      "source": [
        "## ExampleGen\n",
        "The ExampleGen component is responsible for ingesting data in our pipeline in our pipeline as [tf.Examples](https://www.tensorflow.org/tutorials/load_data/tfrecord). We create a separate train and validation split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A180nNdmEToP"
      },
      "source": [
        "# Specify a config for the output with a 3:1 split for the train and eval set:\n",
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=3),\n",
        "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1),\n",
        "    ]))\n",
        "\n",
        "# Define the component:\n",
        "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)\n",
        "\n",
        "# Run the component:\n",
        "start = time()\n",
        "context.run(example_gen)\n",
        "print(f\"time to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9u_HCsZoi26"
      },
      "source": [
        "Lets take a look at the created artifacts. Note that the resulting files are gzipped TFRecord files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z9n6_DhdjCs"
      },
      "source": [
        "artifact_uri = example_gen.outputs['examples'].get()[0].uri\n",
        "\n",
        "print('artifact location:', artifact_uri)\n",
        "print('created artifacts:', os.listdir(artifact_uri))\n",
        "print('created training data files:', os.listdir(os.path.join(artifact_uri, 'Split-train')))\n",
        "print('created validation data files:', os.listdir(os.path.join(artifact_uri, 'Split-eval')))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXI6KpocMHy"
      },
      "source": [
        "## StatisticsGen\n",
        "With the StatisticsGen component we compute descriptive statistics for our dataset. These stats can be visualized for review and are used by the SchemaGen component to infer the schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNDUERjGEyAA"
      },
      "source": [
        "# Create StatisticsGen and run component\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "start = time()\n",
        "context.run(statistics_gen)\n",
        "print(f\"time to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndyjyURSMhtf"
      },
      "source": [
        "Lets take a look at the calculated statistics for our train and eval dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2iv8YqP-wT"
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-noA7Wm7cHKf"
      },
      "source": [
        "# SchemaGen\n",
        "The SchemaGen component infer the schema - data type and range of values for numerical features, range of values for categorical features - based on the statistics from the StatisticsGen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RjOAeJtEyCp"
      },
      "source": [
        "# Create StatisticsGen and run the component\n",
        "schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "\n",
        "start = time()\n",
        "context.run(schema_gen)\n",
        "print(f\"time to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPzcq-DQwOYY"
      },
      "source": [
        "Lets take a look at the outputs produced by SchemaGen. We will see that the data type STRING with all available values in the \"Domain\" is only recognized for some of the categorical features (e.g. \"Company\", \"Memory\") while others are recognized as \"BYTES\". The distinction is made based on the number of different values a categorical feature assumes. However, since both STRING and BYTES are stores as Byteslist in the protobuf fileformat, this distinction is not relevant for subsequent data transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMl72FNfRngq"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgnBDamxb_M5"
      },
      "source": [
        "## ExampleValidator\n",
        "\n",
        "The ExampleValidator component uses dataset statistics and validates them against the data schema. It detects anomalies such as missing values and identifies data-skew and data drift by comparing training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFT9cGZHEyJX"
      },
      "source": [
        "# Create ExampleValidator and run the component:\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "start = time()\n",
        "context.run(example_validator)\n",
        "print(f\"time to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHVrDTwn1Cop"
      },
      "source": [
        "Lets take a look at the output to verify that there are no anomalies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jq5q-CTR9p7"
      },
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39D9nAv_b4J9"
      },
      "source": [
        "# Transform\n",
        "Next we create a Transform component to perform feature engineering. This will result in a TensorFlow graph which will be used to transform tf.Examples for training and serving. To enable this transformation for later inference requests without a Transform component, this graph will be included in the TensorFlow model (SavedModel) that is the result of model training. \n",
        "\n",
        "Unlike the previous components, the transform component requires a custom Python module with a `preprocessing_fn` that defines the transformation using the tensorflow_transform library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izlKttJXh8_B"
      },
      "source": [
        "# lets inspect the number of unique values for each feature. This helps us defining the number of categories\n",
        "# for the one-hot encoding for the categorical features.\n",
        "for col in ds.columns:\n",
        "  print(f'{col}: {ds[col].unique().shape[0]}')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQg_WNz5a45"
      },
      "source": [
        "Before writing the transform_module file, let's save some constants that we require for transformation and later training in a separate `laptop_constants.py` file. Here we specify the numerical and categorical feature keys along with the number of categories to use for the latter. We also sppecify the feature key, which TFX will need to aware of for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tye59lta_Fk5"
      },
      "source": [
        "_laptop_constants_module_file = 'laptop_constants.py'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rL3vRSI_Fm3"
      },
      "source": [
        "%%writefile {_laptop_constants_module_file}\n",
        "\n",
        "MAX_VOCAB_LEN = 25\n",
        "NUMERIC_FEATURE_KEYS = ['Inches', 'ScreenResolution', 'Weight']\n",
        "VOCAB_FEATURE_DICT = {\n",
        "    'Company': 19, \n",
        "    'Product': MAX_VOCAB_LEN, \n",
        "    'TypeName': 6, \n",
        "    'Cpu': MAX_VOCAB_LEN, \n",
        "    'Ram': 9,\n",
        "    'Memory': MAX_VOCAB_LEN, \n",
        "    'Gpu': MAX_VOCAB_LEN, \n",
        "    'OpSys': 9,\n",
        "}\n",
        "NUM_OOV_BUCKETS = 3\n",
        "LABEL_KEY = 'Price_euros'"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7C1ex2Z6G2_"
      },
      "source": [
        "Now we write the `laptop_transform.py` module with the preprocessing function for feature engineering. Take a look at how we transform the strings for ScreenResolution and Weight into a numerical value and how we create one-hot vectors from the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdUwr3tyEyph"
      },
      "source": [
        "_transform_module_file = 'laptop_transform.py'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFKj_r1dgokE"
      },
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import laptop_constants\n",
        "\n",
        "# Unpack the contents of the constants module\n",
        "_NUMERIC_FEATURE_KEYS = laptop_constants.NUMERIC_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_DICT = laptop_constants.VOCAB_FEATURE_DICT\n",
        "_NUM_OOV_BUCKETS = laptop_constants.NUM_OOV_BUCKETS\n",
        "_LABEL_KEY = laptop_constants.LABEL_KEY\n",
        "\n",
        "\n",
        "def _screen_res_width(screen_res):\n",
        "  screen_res = tf.strings.split(screen_res, \"x\")\n",
        "  return tf.reshape(tf.strings.regex_replace(screen_res[0], '[^0-9.]', ''), (1,))\n",
        "\n",
        "# Define the transformations\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    Args:\n",
        "        inputs: map from feature keys to raw not-yet-transformed features.\n",
        "    Returns:\n",
        "        Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize outputs dictionary\n",
        "    outputs = {}\n",
        "    \n",
        "    weight = tf.strings.to_number(\n",
        "        tf.map_fn(\n",
        "            lambda x: tf.strings.regex_replace(x, \"[aA-zZ]\", \"\"),\n",
        "            inputs['Weight']),\n",
        "            out_type=tf.dtypes.float32)\n",
        "\n",
        "    screen_resolution = tf.strings.to_number(\n",
        "          tf.map_fn(\n",
        "          _screen_res_width,\n",
        "          tf.squeeze(inputs['ScreenResolution'], axis=1)),\n",
        "          out_type=tf.dtypes.float32)\n",
        "    \n",
        "    inches = inputs['Inches']\n",
        "\n",
        "    numeric_features_preprocessed = {\n",
        "        'Inches': inches,\n",
        "        'ScreenResolution': screen_resolution,\n",
        "        'Weight': weight\n",
        "    }\n",
        "    \n",
        "    for key, value in numeric_features_preprocessed.items():\n",
        "        scaled = tft.scale_to_0_1(value)\n",
        "        outputs[key] = tf.reshape(scaled, [-1])\n",
        "\n",
        "    # Convert strings to indices and convert to one-hot vectors\n",
        "    for key, vocab_size in _VOCAB_FEATURE_DICT.items():\n",
        "        indices = tft.compute_and_apply_vocabulary(inputs[key], num_oov_buckets=_NUM_OOV_BUCKETS)\n",
        "        one_hot = tf.one_hot(indices, vocab_size + _NUM_OOV_BUCKETS)\n",
        "        outputs[key] = tf.reshape(one_hot, [-1, vocab_size + _NUM_OOV_BUCKETS])\n",
        "\n",
        "    # Cast label to float\n",
        "    outputs[_LABEL_KEY] = tf.cast(inputs[_LABEL_KEY], tf.float32)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZpSTER57Bq0"
      },
      "source": [
        "Now lets create and run the transform component. Notice that in addition to the artifacts from the ExampleGen and SchemaGen component, we pass the created `transform_module.py` as parameter.\n",
        "\n",
        "Running this component might take ~ 50 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsW2TSInEyvK"
      },
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=_transform_module_file)\n",
        "\n",
        "start = time()\n",
        "context.run(transform)\n",
        "print(f\"time to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkP_02nc7wQR"
      },
      "source": [
        "Lets inspect how we transformed our input data by taking a look at one example from our dataset. Note that we are making use of the `tf.data.TFRecordDataset` API which allows us to load, unpack and parse the serialized data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_fMYVAOtm0W"
      },
      "source": [
        "import pprint as pp\n",
        "# Get the URI of the output artifact representing the transformed examples\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Decode the first record and print output\n",
        "for tfrecord in dataset.take(1):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTMN8hTtbtpC"
      },
      "source": [
        "## Trainer\n",
        "Now we are coming to the training part. Note that due to time constraints we skipped the Tuner in this tutorial which would help us to find the best hyperparameters for our model.\n",
        "\n",
        "Similar to the Transform component, we define a `trainer.py` file that we will supply as parameter to the Trainer component. Note the purpose of the functions that we define in the trainer module:\n",
        "\n",
        "*   `_gzip_reader_fn`: to read the compressed data\n",
        "*   `_input_fn`: to create batches of data for training\n",
        "*   `_get_serve_tf_examples_fn`: to parse a serialized tf.Example and apply the transformation from the transform graph obtained from the Transform component\n",
        "*   `model_builder`: create and compile the TensorFlow Keras model\n",
        "*   `run_fn`: define and run the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EdoNt1yHL1W"
      },
      "source": [
        "_trainer_module_file = 'trainer.py'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7SqrxWcHL3j"
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import NamedTuple, Dict, Text, Any, List\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs, DataAccessor\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import laptop_constants\n",
        "\n",
        "_NUMERIC_FEATURE_KEYS = laptop_constants.NUMERIC_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_DICT = laptop_constants.VOCAB_FEATURE_DICT\n",
        "_NUM_OOV_BUCKETS = laptop_constants.NUM_OOV_BUCKETS\n",
        "_LABEL_KEY = laptop_constants.LABEL_KEY\n",
        "\n",
        "N_EPOCHS = 15\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  '''Load compressed dataset\n",
        "  Args: filenames - filenames of TFRecords to load\n",
        "  Returns: TFRecordDataset loaded from the filenames\n",
        "  '''\n",
        "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def _input_fn(file_pattern,\n",
        "              tf_transform_output,\n",
        "              num_epochs=3,\n",
        "              batch_size=32) -> tf.data.Dataset:\n",
        "  '''Create batches of features and labels from TF Records\n",
        "\n",
        "  Args:\n",
        "    file_pattern - List of files or patterns of file paths containing Example records.\n",
        "    tf_transform_output - transform output graph\n",
        "    num_epochs - Integer specifying the number of times to read through the dataset. \n",
        "            If None, cycles through the dataset forever.\n",
        "    batch_size - An int representing the number of records to combine in a single batch.\n",
        "\n",
        "  Returns:\n",
        "    A dataset of dict elements, (or a tuple of dict elements and label). \n",
        "    Each dict maps feature keys to Tensor or SparseTensor objects.\n",
        "  '''\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "  \n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      file_pattern=file_pattern,\n",
        "      batch_size=batch_size,\n",
        "      features=transformed_feature_spec,\n",
        "      reader=_gzip_reader_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      label_key=_LABEL_KEY)\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "\n",
        "  # Get transformation graph\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "    # Get pre-transform feature spec\n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\n",
        "\n",
        "    # Pop label since serving inputs do not include the label\n",
        "    feature_spec.pop(_LABEL_KEY)\n",
        "\n",
        "    # Parse raw examples into a dictionary of tensors matching the feature spec\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "    # Transform the raw examples using the transform graph\n",
        "    transformed_features = model.tft_layer(parsed_features)\n",
        "\n",
        "    # Get predictions using the transformed features\n",
        "    return model(transformed_features)\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def model_builder():\n",
        "  '''\n",
        "  Returns: compiled tf.keras model\n",
        "  '''\n",
        "  # Hyperparameters. Note: these would typically be obtained from the Tuner component\n",
        "  hp_units = 20\n",
        "  hp_learning_rate = 1e-3\n",
        "\n",
        "  # Define input layers for numeric keys\n",
        "  input_tensors_numeric = [\n",
        "      tf.keras.layers.Input(name=colname, shape=(1,), dtype=tf.float32)\n",
        "      for colname in _NUMERIC_FEATURE_KEYS\n",
        "  ]\n",
        "\n",
        "  # Define input layers for vocab keys\n",
        "  input_tensors_categorical = [\n",
        "      tf.keras.layers.Input(name=colname, shape=(vocab_size + _NUM_OOV_BUCKETS,), dtype=tf.float32)\n",
        "      for colname, vocab_size in _VOCAB_FEATURE_DICT.items()\n",
        "  ]\n",
        "\n",
        "  input_tensors = input_tensors_numeric + input_tensors_categorical\n",
        "\n",
        "  # Define the tf.keras model using the Functional API. Start by concatenating the input tensors\n",
        "  x = tf.keras.layers.concatenate(input_tensors)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(1, activation='relu')(x)\n",
        "  output = tf.keras.layers.Lambda(lambda x: x * 2000.0)(x)\n",
        "  model = tf.keras.Model(input_tensors, output)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=tf.keras.losses.MeanSquaredError(),\n",
        "                metrics='mean_absolute_error',\n",
        "                )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "  \"\"\"Defines and trains the model.\n",
        "  Args:\n",
        "    fn_args: Holds args as name/value pairs. Refer here for the complete attributes: \n",
        "    https://www.tensorflow.org/tfx/api_docs/python/tfx/components/trainer/fn_args_utils/FnArgs#attributes\n",
        "  \"\"\"\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch') #batch  epoch\n",
        "  \n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "  \n",
        "  train_set = _input_fn(fn_args.train_files[0], tf_transform_output, 1)\n",
        "  val_set = _input_fn(fn_args.eval_files[0], tf_transform_output, 1)\n",
        "\n",
        "  model = model_builder()\n",
        "\n",
        "  model.fit(\n",
        "      x=train_set,\n",
        "      validation_data=val_set,\n",
        "      callbacks=[tensorboard_callback],\n",
        "      epochs=N_EPOCHS\n",
        "      )\n",
        "  \n",
        "  # Define default serving signature for inference requests\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_serve_tf_examples_fn(model,\n",
        "                                    tf_transform_output).get_concrete_function(\n",
        "                                        tf.TensorSpec(\n",
        "                                            shape=[None],\n",
        "                                            dtype=tf.string,\n",
        "                                            name='examples')),\n",
        "  }\n",
        "  \n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NntdQeoCCxad"
      },
      "source": [
        "Finally, lets create and run the Trainer component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYXACfdQHL55"
      },
      "source": [
        "trainer = Trainer(\n",
        "    module_file=_trainer_module_file,\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval']))\n",
        "\n",
        "start = time()\n",
        "context.run(trainer, enable_cache=False)\n",
        "print(f\"\\ntime to run component: {np.round(time()-start)}s\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IApSwtlQDbOg"
      },
      "source": [
        "Take a look at the progress of training and how the performance improved over 15 epochs. Since this is a regression problem (remember, we predict the prices of laptops), we measure the performance in mean_absolute_error (mean_squared_error for the loss function). Note how the mean absolute error decreases from > 300 to < 200 during training.\n",
        "\n",
        "Lets take a look at the created artifacts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBcUZjb2TccG"
      },
      "source": [
        "# Get artifact uri of trainer model output\n",
        "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
        "print(f'contents of model artifact directory:{os.listdir(model_artifact_dir)}')\n",
        "\n",
        "model_dir = os.path.join(model_artifact_dir, 'Format-Serving')\n",
        "print(f'contents of model directory: {os.listdir(model_dir)}')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HsFtB8fEtBt"
      },
      "source": [
        "(Optional:) To inspect performance more thoroughly, load and take a look at the Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHRtZFs-Tyz0"
      },
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BVU7mb-c1TP"
      },
      "source": [
        "## Evaluator\n",
        "\n",
        "With the Evaluator component we verify if the model performance is satisfactory for pushing to deployment. The Evaluator runs the trained model with validation data (raw eval split from ExampleGen) and “blesses” the trained model if it passes the criteria specified in the eval config\n",
        "\n",
        "Lets first specify the eval config. Notice that we specify an upper bound of 300 as threshold for the MeanAbsoluteError. You can play around with this value. If you set it too low (e.g. 100) the model will likely not pass the validation and will consequently not be blessed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DPMJwScTy1z"
      },
      "source": [
        "eval_config = text_format.Parse(\"\"\"\n",
        "  ## Model information\n",
        "  model_specs {\n",
        "    # This assumes a serving model with signature 'serving_default'.\n",
        "    signature_name: \"serving_default\",\n",
        "    label_key: \"Price_euros\"\n",
        "  }\n",
        "\n",
        "  ## Post training metric information\n",
        "  metrics_specs {\n",
        "    metrics { class_name: \"ExampleCount\" }\n",
        "    metrics {\n",
        "      class_name: \"MeanAbsoluteError\"\n",
        "      threshold {\n",
        "        # Ensure that metric is always < XXX\n",
        "        value_threshold {\n",
        "          upper_bound { value: 300 }\n",
        "        }\n",
        "        # Ensure that metric does not drop by more than a small epsilon\n",
        "        # e.g. (candidate - baseline) > -1e-10 or candidate > baseline - 1e-10\n",
        "        change_threshold {\n",
        "          direction: LOWER_IS_BETTER\n",
        "          absolute { value: 1e4 }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    metrics { class_name: \"MeanSquaredError\" }\n",
        "  }\n",
        "\n",
        "  ## Slicing information\n",
        "  slicing_specs {}  # overall slice\n",
        "\n",
        "\"\"\", tfma.EvalConfig())"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aegjvXVHkKK"
      },
      "source": [
        "Here we define the model resolver who is responsible for selecting the right model. We use the `LatestBlessedModelStrategy` which, as the name suggests, picks the latest trained model among all models that passed the validation of the Evaluator.\n",
        "\n",
        "Since we haven't run the Evaluator component yet, the output artifact will show empty results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szKJU5FpTy4N"
      },
      "source": [
        "# Setup the Resolver node to find the latest blessed model\n",
        "model_resolver = tfx_v1.dsl.Resolver(\n",
        "      strategy_class=tfx_v1.dsl.experimental.LatestBlessedModelStrategy,\n",
        "      model=tfx_v1.dsl.Channel(type=tfx_v1.types.standard_artifacts.Model),\n",
        "      model_blessing=tfx_v1.dsl.Channel(\n",
        "          type=tfx_v1.types.standard_artifacts.ModelBlessing)).with_id(\n",
        "              'latest_blessed_model_resolver')\n",
        "\n",
        "# Run the Resolver node\n",
        "context.run(model_resolver)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cczvJriCTy6R"
      },
      "source": [
        "# Load Resolver outputs\n",
        "model_resolver.outputs['model']"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZgXG-y6I7GA"
      },
      "source": [
        "Now we run the Evaluator. Note that we pass the model resolver output model as \"baseline\" model to the component. What this does is it compares the model produced by the training run to the latest blessed model (none in our case because we run the component for the first time) and picks the better performing one. This ensures that a newly trained model only gets pushed if its performance exceeds a baseline model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-LCmqEDTy8h"
      },
      "source": [
        "# Setup and run the Evaluator component\n",
        "evaluator = tfx.components.Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "\n",
        "context.run(evaluator, enable_cache=False)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn7nxZSYJ6Li"
      },
      "source": [
        "Lets verify that the Evaluator was able to produce a blessed model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDLmkWsKfvQ4"
      },
      "source": [
        "# Get `Evaluator` blessing output URI\n",
        "blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n",
        "os.listdir(blessing_uri)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP8-kk4UKGOa"
      },
      "source": [
        "We can also visualize the Evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBZmtNg8fvUl"
      },
      "source": [
        "# Visualize the evaluation results\n",
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQUBcUwAfek"
      },
      "source": [
        "## Pusher\n",
        "\n",
        "Finally, we push our model to the serving model directory. Note that the pusher does not take care of serving the model but just pushes it to the appropriate destination.\n",
        "\n",
        "To serve the model for inference requests we use TensorFlow Serving. This will be covered in the second exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00PGK6kIxFsq"
      },
      "source": [
        "# Setup serving path\n",
        "import tempfile\n",
        "\n",
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(),\n",
        "                                  'serving_model')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZjEF1w2r7oc"
      },
      "source": [
        "pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=_serving_model_dir)))\n",
        "\n",
        "context.run(pusher)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BBtOyjbLJ2f"
      },
      "source": [
        "Congrats! You explored all (well excpept for the Tuner) TFX components and ran them successfully. You were able to run and inspect them individually using the InteractiveContext.\n",
        "\n",
        "In the next exercise we will run this complete identical pipeline from beginning to end and inspect the ML Metadata store as well as TF Serving"
      ]
    }
  ]
}