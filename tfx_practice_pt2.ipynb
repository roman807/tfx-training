{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfx_practice_pt2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaTEXBlN0zae"
      },
      "source": [
        "# Hands-on Exercise Part 2: ML Metadata and TF Serving\n",
        "\n",
        "And this second exercise we will run the complete pipeline using the same code as in the previous exercise. This time we clone it directly and execute the complete pipeline with the LocalDagRunner. We will also take a look at the ML Metadata store and TF Serving.\n",
        "\n",
        "But first, lets again install TFX and *RESTART RUNTIME* thereafter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lHo_DC0fx8T"
      },
      "source": [
        "! pip install -U tfx"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfC3rxSKNl8_"
      },
      "source": [
        "Lets now import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84tA-Sjkg_Nj"
      },
      "source": [
        "import tfx\n",
        "from tfx import v1 as tfxv1\n",
        "import os\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHudP1kn1-fu"
      },
      "source": [
        "## 1. Complete pipeleine run with LocalDagRunner\n",
        "\n",
        "To obtain the same setting as at the end of th previous exercise, we run the complete pipeline with the LocalDagRunner. The code for the individual components is identical and we clone it directly from Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DVgqxvFgucE"
      },
      "source": [
        "! git clone https://github.com/roman807/tfx-training.git"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gInBLEsNy1V"
      },
      "source": [
        "We can now cd into the tfx-training directory and take a look at the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0URULt0lhSBl"
      },
      "source": [
        "os.chdir('tfx-training')\n",
        "os.listdir()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfo7DiZyOEK8"
      },
      "source": [
        "Next, we:\n",
        "\n",
        "1.   Import the create_pipeline function from the `laptop_pipeline.py` module\n",
        "2.   Define the required paths\n",
        "3.   Run the complete pipeline locally with the LocalDagRunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOVQ3sXJgxxy"
      },
      "source": [
        "from laptop_pipeline import create_pipeline\n",
        "\n",
        "DATA_ROOT = 'data'\n",
        "SERVING_MODEL_DIR = 'serving_model'                      \n",
        "PIPELINE_NAME = 'laptop_pipeline'   # where we will save the final model for deployment with TF serving\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "\n",
        "start = time()\n",
        "tfxv1.orchestration.LocalDagRunner().run(\n",
        "  create_pipeline(\n",
        "      data_root=DATA_ROOT,\n",
        "      transform_module_file='utils/transform_module.py',\n",
        "      trainer_module_file='utils/trainer_module.py',\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH,\n",
        "      ))\n",
        "print(f'\\ncompleted pipeline run in  {np.round(time()-start)}s')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN3YXdFfO4PB"
      },
      "source": [
        "Lets confirm that all the components are here as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qs20MVKzqPX"
      },
      "source": [
        "os.listdir('pipelines/laptop_pipeline')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFLIsuTvuj4W"
      },
      "source": [
        "## 2. ML Metadata store\n",
        "\n",
        "When we ran our pipeline with the LocalDagRunner, the runner created the ML Metadata store locally at our specified path as a SQLite database. Let's create a connection to the metadata store and explore the content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmJ8agAuhKqb"
      },
      "source": [
        "import ml_metadata as mlmd\n",
        "from ml_metadata.metadata_store import metadata_store\n",
        "from ml_metadata.proto import metadata_store_pb2\n",
        "\n",
        "connection_config = metadata_store_pb2.ConnectionConfig()\n",
        "connection_config.sqlite.filename_uri = METADATA_PATH\n",
        "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
        "store = metadata_store.MetadataStore(connection_config)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY0JKmvpTW61"
      },
      "source": [
        "See the various available calls for the metadata store object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i8ZXwS_STno"
      },
      "source": [
        "[call for call in dir(store) if '__' not in call]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaOTxtCSRPSm"
      },
      "source": [
        "As a reminder:\n",
        "*  Artifacts: information about inputs/outputs\n",
        "*  Executions: records of component runs & runtime parameters\n",
        "*  Context: conceptual group of artifacts and executions in a workflow\n",
        "\n",
        "Explore the Artifacts, Executions and Contexts by individually uncommenting them and running the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmmbyBERPqX"
      },
      "source": [
        "# store.get_artifact_types()\n",
        "# store.get_artifacts()\n",
        "# store.get_artifacts_by_type('Examples')\n",
        "# store.get_artifacts_by_type('Schema')\n",
        "# store.get_artifacts_by_type('#####')\n",
        "\n",
        "# store.get_execution_types()\n",
        "# store.get_executions()\n",
        "\n",
        "# store.get_contexts()  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKYtI5REsV8f"
      },
      "source": [
        "### Get the schema for later inference requests\n",
        "Now, let's make use of the metadata store for the next part of this exercise. For an inference request with TF serving we'll need the schema of the raw data samples (tf.Examples).\n",
        "\n",
        "We first get the location of the schema from the metadata store and then load and parse the schema.pbtxt file with tfx io_utils "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpBkmRYVq-aK"
      },
      "source": [
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "# get the schema uri from the metadata store:\n",
        "schema_uri = store.get_artifacts_by_type('Schema')[0].uri\n",
        "\n",
        "# load and parse the schema:\n",
        "schema_filename = os.path.join(schema_uri, \"schema.pbtxt\")\n",
        "schema = tfx.utils.io_utils.parse_pbtxt_file(file_name=schema_filename,\n",
        "                                             message=schema_pb2.Schema())"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFGAL-Ixl0i_"
      },
      "source": [
        "## 3. TensorFlow Serving\n",
        "\n",
        "Finally, lets explore how to spin up a server with TensorFlow Serving and deploy a model for inference requests\n",
        "\n",
        "First, we install tensorflow-serving using command line tools:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQWg6Hb5dYoi"
      },
      "source": [
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PQrLzDndYx0"
      },
      "source": [
        "! apt-get install tensorflow-model-server"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4oZEYhFVAnK"
      },
      "source": [
        "Lets now set the location of our latest pushed model as an environment variable for the server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEx20tHadYz8"
      },
      "source": [
        "latest_pushed_model = os.path.join(SERVING_MODEL_DIR, max(os.listdir(SERVING_MODEL_DIR)))\n",
        "os.environ[\"MODEL_DIR\"] = os.path.join(os.getcwd(), os.path.split(latest_pushed_model)[0])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNi5opPMVVwL"
      },
      "source": [
        "Spin up TF serving server on localhost / port 8501"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ9b9_CsfIkC"
      },
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=laptop_price_predictor \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eycTZYvTViiO"
      },
      "source": [
        "Lets take a look at the server.log to verify that our server runs as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INtELtU_fImZ"
      },
      "source": [
        "! tail server.log"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia-Xz5oMVsmy"
      },
      "source": [
        "Now we define helper functions to prepare a sample for inference. We use the TF Serving REST API that expects requests in JSON format. Meanwhile our TensorFlow model expects input in Protobuf format.\n",
        "\n",
        "Therefore, we first serialize our input to Protobuf (`_make_serialized_examples`), then encode our examples with a b64-encoder for JSON serialization and send them to the server with the requests library (`do_inference`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtq6Jyryfppd"
      },
      "source": [
        "import base64\n",
        "import json\n",
        "import requests\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "from tensorflow_transform import coders as tft_coders\n",
        "\n",
        "import laptop_constants\n",
        "\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "def _make_proto_coder(schema):\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_schema = schema_utils.schema_from_feature_spec(raw_feature_spec)\n",
        "  return tft_coders.ExampleProtoCoder(raw_schema)\n",
        "\n",
        "def make_serialized_examples(example_jsons, schema):\n",
        "  \"\"\"Parses examples from CSV file and returns seralized proto examples.\"\"\"\n",
        "  filtered_features = [\n",
        "      feature for feature in schema.feature if feature.name != laptop_constants.LABEL_KEY\n",
        "  ]\n",
        "  del schema.feature[:]\n",
        "  schema.feature.extend(filtered_features)\n",
        "\n",
        "  proto_coder = _make_proto_coder(schema)\n",
        "\n",
        "  serialized_examples = []\n",
        "  for sample in example_jsons:\n",
        "    one_example = {}\n",
        "    for feature in schema.feature:\n",
        "      name = feature.name\n",
        "      if sample[name]:\n",
        "        if feature.type == schema_pb2.FLOAT:\n",
        "          one_example[name] = [float(sample[name])]\n",
        "        elif feature.type == schema_pb2.INT:\n",
        "          one_example[name] = [int(sample[name])]\n",
        "        elif feature.type == schema_pb2.BYTES:\n",
        "          one_example[name] = [sample[name].encode('utf8')]\n",
        "      else:\n",
        "        one_example[name] = []\n",
        "\n",
        "    serialized_example = proto_coder.encode(one_example)\n",
        "    serialized_examples.append(serialized_example)\n",
        "  \n",
        "  return serialized_examples"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMIdbU6tfpr8"
      },
      "source": [
        "def do_inference(server_addr, model_name, serialized_examples):\n",
        "  \"\"\"Sends requests to the model and prints the results.\n",
        "  Args:\n",
        "    server_addr: network address of model server in \"host:port\" format\n",
        "    model_name: name of the model as understood by the model server\n",
        "    serialized_examples: serialized examples of data to do inference on\n",
        "  \"\"\"\n",
        "  parsed_server_addr = server_addr.split(':')\n",
        "\n",
        "  host=parsed_server_addr[0]\n",
        "  port=parsed_server_addr[1]\n",
        "  json_examples = []\n",
        "  \n",
        "  for serialized_example in serialized_examples:\n",
        "    # The encoding follows the guidelines in:\n",
        "    # https://www.tensorflow.org/tfx/serving/api_rest\n",
        "    example_bytes = base64.b64encode(serialized_example).decode('utf-8')\n",
        "    predict_request = '{ \"b64\": \"%s\" }' % example_bytes\n",
        "    json_examples.append(predict_request)\n",
        "\n",
        "  json_request = '{ \"instances\": [' + ','.join(map(str, json_examples)) + ']}'\n",
        "\n",
        "  server_url = 'http://' + host + ':' + port + '/v1/models/' + model_name + ':predict'\n",
        "  response = requests.post(\n",
        "      server_url, data=json_request, timeout=5.0)\n",
        "  response.raise_for_status()\n",
        "  prediction = response.json()\n",
        "  print(json.dumps(prediction, indent=4))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuFcdVwXTCn"
      },
      "source": [
        "Lets define an example of a laptop and predict the price. Play around by changing some Paramers (e.g. Company from Apple -> Lenovo, Inches increase, different Ram, etc.) and see how the estimated price changes accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7O1W9CKmwFV"
      },
      "source": [
        "example_jsons = [\n",
        "  {\n",
        "      'laptop_ID': 1, \n",
        "      'Company': 'Apple', \n",
        "      'Product': 'MacBook',\n",
        "      'TypeName': 'Ultrabook',\n",
        "      'Inches': 13.3, \n",
        "      'ScreenResolution': '1999x900', \n",
        "      'Cpu': 'Intel Core i5 2.3GHz',\n",
        "      'Ram': '8GB', \n",
        "      'Memory': '256GB SSD', \n",
        "      'Gpu': 'Intel Iris Plus Graphics 640', \n",
        "      'OpSys': 'macOS', \n",
        "      'Weight': '1.9kg', \n",
        "   \n",
        "  }\n",
        "]\n",
        "serialized_examples = make_serialized_examples(\n",
        "    example_jsons=example_jsons,\n",
        "    schema=schema)\n",
        "\n",
        "do_inference(server_addr='127.0.0.1:8501', \n",
        "     model_name='laptop_price_predictor',\n",
        "     serialized_examples=serialized_examples)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUZPVW0KYPVf"
      },
      "source": [
        "Congrats! You finished this exercise and saw how to run a complete pipeline, use the ML metadatastore to retrieve artifacts and run inference with TF Serving"
      ]
    }
  ]
}